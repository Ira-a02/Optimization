Программа использует три разных метода оптимизации для нахождения минимума функции двух переменных. Каждый из методов имеет свои особенности и может давать разные результаты. 
Ищется экстремум функция (x1 + 6 * x2)^2 + (x1 + 2)^2.
1. Метод градиентного спуска с дробным шагом
Принцип работы:

Метод градиентного спуска используется для нахождения минимума функции, основываясь на значениях её градиента.
Начинается с произвольной точки (x1, x2).
На каждой итерации вычисляется градиент функции, который указывает направление наибольшего увеличения функции.
Для нахождения минимума необходимо двигаться в противоположном направлении градиента.
Шаг, на который мы двигаемся, может быть уменьшен (дробный шаг), чтобы обеспечить более точное приближение к минимуму.
Процесс:

Вычисляется градиент функции в текущей точке.
Обновляются значения переменных:
x1new=x1old−α⋅∂F/∂x1
x2new=x2old−α⋅∂F/∂x2
​где α — это дробный шаг.
Повторяется, пока не будет достигнуто условие остановки (например, значение функции перестает существенно меняться).
2. Метод Марквардта
Принцип работы:

Метод Марквардта (или метод наименьших квадратов) является более продвинутым методом оптимизации, который может использовать информацию о второй производной (гессиан) для улучшения сходимости.
Он ищет минимум функции, используя как градиент, так и кривизну функции.
Процесс:

Как и в градиентном спуске, начинается с начальной точки и вычисляется градиент.
Вместо простого обновления значений переменных, метод использует информацию о гессиане, чтобы более эффективно находить направление для обновления.
Обновление переменных происходит с учетом как градиента, так и гессиана, что позволяет быстрее сходиться к минимуму.
3. Метод быстрого градиента
Принцип работы:

Метод быстрого градиента (или Nesterov accelerated gradient) является улучшением стандартного метода градиентного спуска.
Он использует информацию о предыдущих итерациях для более быстрого нахождения минимума, что позволяет избежать некоторых проблем, связанных с медленной сходимостью.
Процесс:

На каждой итерации вычисляется "предварительное" положение, основываясь на предыдущих шагах.
Затем вычисляется градиент в этом новом положении.
Обновляется текущее положение с учетом как текущего градиента, так и информации о предыдущих итерациях, что позволяет делать более "умные" шаги.
